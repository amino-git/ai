{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e0a1bdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.48892529\n",
      "Iteration 2, loss = 0.34278636\n",
      "Iteration 3, loss = 0.26010966\n",
      "Iteration 4, loss = 0.17342444\n",
      "Iteration 5, loss = 0.10914229\n",
      "Iteration 6, loss = 0.07412748\n",
      "Iteration 7, loss = 0.05882393\n",
      "Iteration 8, loss = 0.05119724\n",
      "Iteration 9, loss = 0.04763446\n",
      "Iteration 10, loss = 0.04500683\n",
      "Iteration 11, loss = 0.04345088\n",
      "Iteration 12, loss = 0.04219227\n",
      "Iteration 13, loss = 0.04127707\n",
      "Iteration 14, loss = 0.04128120\n",
      "Iteration 15, loss = 0.04043555\n",
      "Iteration 16, loss = 0.03968348\n",
      "Iteration 17, loss = 0.03893305\n",
      "Iteration 18, loss = 0.03932761\n",
      "Iteration 19, loss = 0.03908522\n",
      "Iteration 20, loss = 0.03814079\n",
      "Iteration 21, loss = 0.03791258\n",
      "Iteration 22, loss = 0.03768116\n",
      "Iteration 23, loss = 0.03756063\n",
      "Iteration 24, loss = 0.03744141\n",
      "Iteration 25, loss = 0.03726987\n",
      "Iteration 26, loss = 0.03696747\n",
      "Iteration 27, loss = 0.03646602\n",
      "Iteration 28, loss = 0.03642213\n",
      "Iteration 29, loss = 0.03673453\n",
      "Iteration 30, loss = 0.03627997\n",
      "Iteration 31, loss = 0.03638670\n",
      "Iteration 32, loss = 0.03658557\n",
      "Iteration 33, loss = 0.03576977\n",
      "Iteration 34, loss = 0.03587123\n",
      "Iteration 35, loss = 0.03562702\n",
      "Iteration 36, loss = 0.03552915\n",
      "Iteration 37, loss = 0.03506470\n",
      "Iteration 38, loss = 0.03544364\n",
      "Iteration 39, loss = 0.03468459\n",
      "Iteration 40, loss = 0.03476546\n",
      "Iteration 41, loss = 0.03450379\n",
      "Iteration 42, loss = 0.03446975\n",
      "Iteration 43, loss = 0.03430334\n",
      "Iteration 44, loss = 0.03417161\n",
      "Iteration 45, loss = 0.03426118\n",
      "Iteration 46, loss = 0.03409443\n",
      "Iteration 47, loss = 0.03397381\n",
      "Iteration 48, loss = 0.03368520\n",
      "Iteration 49, loss = 0.03356098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\anaconda3\\anac\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 50, loss = 0.03318481\n",
      "\n",
      "دقة النموذج على الاختبار: 0.774\n",
      "\n",
      "تقرير التصنيف:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.74      0.85      0.79      5768\n",
      "    Positive       0.82      0.70      0.75      5752\n",
      "\n",
      "    accuracy                           0.77     11520\n",
      "   macro avg       0.78      0.77      0.77     11520\n",
      "weighted avg       0.78      0.77      0.77     11520\n",
      "\n",
      "\n",
      "ادخل تغريدة جديدة (اكتب 'exit' للخروج):\n",
      ">>> كيف حالك\n",
      "➡️ التغريدة سلبية\n",
      "\n",
      "ادخل تغريدة جديدة (اكتب 'exit' للخروج):\n",
      ">>> exit\n",
      "تم إنهاء البرنامج.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.utils import resample\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.isri import ISRIStemmer\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "\n",
    "# 2️⃣ ملفات التدريب والاختبار\n",
    "train_files = [\n",
    "    r\"C:\\Users\\PC\\Desktop\\ai\\train_Arabic_tweets_negative_20190413.tsv\",\n",
    "    r\"C:\\Users\\PC\\Desktop\\ai\\train_Arabic_tweets_positive_20190413.tsv\"\n",
    "]\n",
    "test_files = [\n",
    "    r\"C:\\Users\\PC\\Desktop\\ai\\test_Arabic_tweets_negative_20190413.tsv\",\n",
    "    r\"C:\\Users\\PC\\Desktop\\ai\\test_Arabic_tweets_positive_20190413.tsv\"\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# 3️⃣ دالة تنظيف النصوص\n",
    "stemmer = ISRIStemmer()\n",
    "arabic_stopwords = set(stopwords.words('arabic'))\n",
    "\n",
    "def clean_text(text):\n",
    "    text = str(text)\n",
    "    text = re.sub(r\"http\\S+|www.\\S+\", \"\", text)   # إزالة الروابط\n",
    "    text = re.sub(r\"@\\w+\", \"\", text)               # إزالة @\n",
    "    text = re.sub(r\"#\", \"\", text)                  # إزالة #\n",
    "    text = re.sub(r\"[^\\u0600-\\u06FF\\s]\", \"\", text) # إزالة الرموز والأرقام\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    words = [stemmer.stem(w) for w in text.split() if w not in arabic_stopwords]\n",
    "    return \" \".join(words)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 4️⃣ قراءة الملفات وتنظيفها\n",
    "def read_and_clean(files):\n",
    "    all_data = []\n",
    "    for file in files:\n",
    "        df = pd.read_csv(file, sep=\"\\t\", header=None, names=[\"label\", \"text\"])\n",
    "        df[\"label\"] = df[\"label\"].map({\"neg\": 0, \"pos\": 1})\n",
    "        df[\"clean_text\"] = df[\"text\"].apply(clean_text)\n",
    "        all_data.append(df[[\"clean_text\", \"label\"]])\n",
    "    return pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "train_df = read_and_clean(train_files)\n",
    "test_df = read_and_clean(test_files)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 5️⃣ توازن الفئات (oversampling)\n",
    "df_majority = train_df[train_df.label==0]\n",
    "df_minority = train_df[train_df.label==1]\n",
    "\n",
    "df_minority_upsampled = resample(df_minority, \n",
    "                                 replace=True,     \n",
    "                                 n_samples=len(df_majority),    \n",
    "                                 random_state=42)\n",
    "\n",
    "train_df_balanced = pd.concat([df_majority, df_minority_upsampled])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 6️⃣ TF-IDF محسّن\n",
    "vectorizer = TfidfVectorizer(max_features=15000, ngram_range=(1,2))\n",
    "X_train = vectorizer.fit_transform(train_df_balanced[\"clean_text\"])\n",
    "X_test = vectorizer.transform(test_df[\"clean_text\"])\n",
    "y_train = train_df_balanced[\"label\"]\n",
    "y_test = test_df[\"label\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 7️⃣ تدريب شبكة عصبية أكبر ومحسنة\n",
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(150, 50),\n",
    "    max_iter=50,\n",
    "    random_state=42,\n",
    "    verbose=True,\n",
    "    learning_rate_init=0.001\n",
    ")\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 8️⃣ اختبار النموذج\n",
    "y_pred = mlp.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"\\nدقة النموذج على الاختبار:\", round(accuracy, 4))\n",
    "print(\"\\nتقرير التصنيف:\")\n",
    "print(classification_report(y_test, y_pred, target_names=[\"Negative\", \"Positive\"]))           \n",
    "\n",
    "\n",
    "\n",
    "# # حفظ النموذج و TF-IDF Vectorizer\n",
    "# import joblib\n",
    "# joblib.dump(mlp, \"mlp_model.pkl\")\n",
    "# joblib.dump(vectorizer, \"tfidf_vectorizer.pkl\")\n",
    "# print(\"تم حفظ النموذج وVectorizer جاهزين للاستخدام في الويب!\")\n",
    "\n",
    "\n",
    "# mlp = joblib.load(\"mlp_model.pkl\")\n",
    "# vectorizer = joblib.load(\"tfidf_vectorizer.pkl\")\n",
    "\n",
    "\n",
    "\n",
    "# 9️⃣ واجهة تجربة التغريدات الجديدة\n",
    "while True:\n",
    "    tweet = input(\"\\nادخل تغريدة جديدة (اكتب 'exit' للخروج):\\n>>> \")\n",
    "    if tweet.lower() == \"exit\":\n",
    "        print(\"تم إنهاء البرنامج.\")\n",
    "        break\n",
    "    tweet_clean = clean_text(tweet)\n",
    "    tweet_vector = vectorizer.transform([tweet_clean])\n",
    "    pred = mlp.predict(tweet_vector)[0]\n",
    "    print(\"➡️ التغريدة إيجابية\" if pred == 1 else \"➡️ التغريدة سلبية\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bb1262b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "تم حفظ النموذج وVectorizer جاهزين للاستخدام في الويب!\n"
     ]
    }
   ],
   "source": [
    "# حفظ النموذج و TF-IDF Vectorizer\n",
    "import joblib\n",
    "joblib.dump(mlp, \"mlp_model.pkl\")\n",
    "joblib.dump(vectorizer, \"tfidf_vectorizer.pkl\")\n",
    "print(\"تم حفظ النموذج وVectorizer جاهزين للاستخدام في الويب!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6b56d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ادخل تغريدة جديدة (اكتب 'exit' للخروج):\n",
      ">>> انت كلب\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'clean_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mتم إنهاء البرنامج.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m tweet_clean \u001b[38;5;241m=\u001b[39m \u001b[43mclean_text\u001b[49m(tweet)\n\u001b[0;32m      8\u001b[0m tweet_vector \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mtransform([tweet_clean])\n\u001b[0;32m      9\u001b[0m pred \u001b[38;5;241m=\u001b[39m mlp\u001b[38;5;241m.\u001b[39mpredict(tweet_vector)[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'clean_text' is not defined"
     ]
    }
   ],
   "source": [
    "# 9️⃣ واجهة تجربة التغريدات الجديدة\n",
    "while True:\n",
    "    tweet = input(\"\\nادخل تغريدة جديدة (اكتب 'exit' للخروج):\\n>>> \")\n",
    "    if tweet.lower() == \"exit\":\n",
    "        print(\"تم إنهاء البرنامج.\")\n",
    "        break\n",
    "    tweet_clean = clean_text(tweet)\n",
    "    tweet_vector = vectorizer.transform([tweet_clean])\n",
    "    pred = mlp.predict(tweet_vector)[0]\n",
    "    print(\"➡️ التغريدة إيجابية\" if pred == 1 else \"➡️ التغريدة سلبية\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d0b6971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ادخل تغريدة جديدة (اكتب 'exit' للخروج):\n",
      ">>> انت جميل\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'clean_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mتم إنهاء البرنامج.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m tweet_clean \u001b[38;5;241m=\u001b[39m \u001b[43mclean_text\u001b[49m(tweet)\n\u001b[0;32m      8\u001b[0m tweet_vector \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mtransform([tweet_clean])\n\u001b[0;32m      9\u001b[0m pred \u001b[38;5;241m=\u001b[39m mlp\u001b[38;5;241m.\u001b[39mpredict(tweet_vector)[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'clean_text' is not defined"
     ]
    }
   ],
   "source": [
    "# 9️⃣ واجهة تجربة التغريدات الجديدة\n",
    "while True:\n",
    "    tweet = input(\"\\nادخل تغريدة جديدة (اكتب 'exit' للخروج):\\n>>> \")\n",
    "    if tweet.lower() == \"exit\":\n",
    "        print(\"تم إنهاء البرنامج.\")\n",
    "        break\n",
    "    tweet_clean = clean_text(tweet)\n",
    "    tweet_vector = vectorizer.transform([tweet_clean])\n",
    "    pred = mlp.predict(tweet_vector)[0]\n",
    "    print(\"➡️ التغريدة إيجابية\" if pred == 1 else \"➡️ التغريدة سلبية\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9341be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
