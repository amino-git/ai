import streamlit as st
import pandas as pd
import joblib
import re
import os
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.neural_network import MLPClassifier
from sklearn.utils import resample
from nltk.corpus import stopwords
from nltk.stem.isri import ISRIStemmer
import nltk

# Ù…ÙƒØªØ¨Ø§Øª Google Drive Ø¹Ø¨Ø± OAuth
from google_auth_oauthlib.flow import InstalledAppFlow
from googleapiclient.discovery import build
from googleapiclient.http import MediaFileUpload

# ØªØ­Ù…ÙŠÙ„ stopwords Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
nltk.download('stopwords', quiet=True)

# ---------------------------
# Ø¥Ø¹Ø¯Ø§Ø¯ Google Drive (OAuth)
# ---------------------------
SCOPES = ['https://www.googleapis.com/auth/drive']
CREDENTIALS_JSON = "credentials.json"  # Ù…Ù„Ù Client ID Ùˆ Client Secret Ù…Ù† Google Cloud

# Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ token Ø³Ø§Ø¨Ù‚
if os.path.exists("token.pkl"):
    creds = joblib.load("token.pkl")
else:
    flow = InstalledAppFlow.from_client_secrets_file(CREDENTIALS_JSON, SCOPES)
    creds = flow.run_local_server(port=0)  # ÙŠÙØªØ­ Ù†Ø§ÙØ°Ø© Ù„ØªØ³Ø¬ÙŠÙ„ Ø¯Ø®ÙˆÙ„ Gmail
    joblib.dump(creds, "token.pkl")  # Ø­ÙØ¸ Ø§Ù„ØªÙˆÙƒÙ† Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø§Ø­Ù‚Ù‹Ø§

drive_service = build('drive', 'v3', credentials=creds)

# Ù…Ø¹Ø±Ù Ø§Ù„Ù…Ø¬Ù„Ø¯ ÙÙŠ Google Drive (Ø¶Ø¹Ù‡ Ø¥Ø°Ø§ Ø£Ø±Ø¯Øª Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ Ù…Ø­Ø¯Ø¯)
DRIVE_FOLDER_ID = "1mOXjtLO5q6lKgt8cCeVBlGVZdIhTOl7W"  # None ÙŠØ¹Ù†ÙŠ Ø±ÙØ¹Ù‡Ø§ ÙÙŠ My Drive Ù…Ø¨Ø§Ø´Ø±Ø©

def upload_to_drive(local_file, drive_folder_id=DRIVE_FOLDER_ID):
    try:
        file_name = os.path.basename(local_file)
        media = MediaFileUpload(local_file, resumable=True)

        if drive_folder_id:
            query = f"name='{file_name}' and '{drive_folder_id}' in parents and trashed=false"
            result = drive_service.files().list(q=query, fields="files(id, name)").execute()
            files = result.get('files', [])
        else:
            files = []

        if files:
            file_id = files[0]['id']
            drive_service.files().update(fileId=file_id, media_body=media).execute()
        else:
            file_metadata = {"name": file_name}
            if drive_folder_id:
                file_metadata["parents"] = [drive_folder_id]
            drive_service.files().create(body=file_metadata, media_body=media, fields="id").execute()

        st.success(f"âœ… ØªÙ… Ø±ÙØ¹ {file_name} Ø¨Ù†Ø¬Ø§Ø­ Ø¥Ù„Ù‰ Google Drive!")
    except Exception as e:
        st.error(f"âŒ ÙØ´Ù„ Ø±ÙØ¹ {local_file} Ø¥Ù„Ù‰ Google Drive:\n{e}")

# ---------------------------
# ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø¬Ù„Ø³Ø©
# ---------------------------
if "logged_in" not in st.session_state:
    st.session_state.logged_in = False
if "mlp" not in st.session_state:
    st.session_state.mlp = None
if "vectorizer" not in st.session_state:
    st.session_state.vectorizer = None
if "dark_mode" not in st.session_state:
    st.session_state.dark_mode = False

# ---------------------------
# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…ÙˆØ¬ÙˆØ¯Ø§Ù‹
# ---------------------------
if st.session_state.mlp is None or st.session_state.vectorizer is None:
    if os.path.exists("mlp_model.pkl") and os.path.exists("tfidf_vectorizer.pkl"):
        st.session_state.mlp = joblib.load("mlp_model.pkl")
        st.session_state.vectorizer = joblib.load("tfidf_vectorizer.pkl")

# ---------------------------
# Ø¯Ø§Ù„Ø© ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ
# ---------------------------
stemmer = ISRIStemmer()
arabic_stopwords = set(stopwords.words('arabic'))

def clean_text(text, lang="ar"):
    text = str(text).lower()
    text = re.sub(r"http\S+|www.\S+", "", text)
    text = re.sub(r"@\w+", "", text)
    text = re.sub(r"#", "", text)
    text = re.sub(r"\d+", "", text)
    text = re.sub(r"[^\w\s\u0600-\u06FF]", "", text)
    text = re.sub(r"\s+", " ", text).strip()
    if lang == "ar":
        words = [stemmer.stem(w) for w in text.split() if w not in arabic_stopwords]
    else:
        words = [w for w in text.split() if len(w) > 1]
    return " ".join(words)

# ---------------------------
# ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
# ---------------------------
st.title("Amily ğŸ“")
st.markdown("---")

st.subheader("ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…")
user_tweet = st.text_input("Ø§Ø¯Ø®Ù„ ØªØºØ±ÙŠØ¯ØªÙƒ Ù‡Ù†Ø§")
lang = st.radio("Ø§Ø®ØªØ± Ø§Ù„Ù„ØºØ©", ["Arabic", "English"], key="user_lang")

if st.button("ØµÙ†Ù Ø§Ù„ØªØºØ±ÙŠØ¯Ø© (Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…)"):
    if st.session_state.mlp and st.session_state.vectorizer:
        tweet_clean = clean_text(user_tweet, lang="ar" if lang=="Arabic" else "en")
        if tweet_clean.strip() == "":
            st.warning("âš ï¸ Ø§Ù„Ù†Øµ Ù„Ø§ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ ÙƒÙ„Ù…Ø§Øª ØµØ§Ù„Ø­Ø© Ù„Ù„ØªØµÙ†ÙŠÙ.")
        else:
            tweet_vector = st.session_state.vectorizer.transform([tweet_clean])
            pred = st.session_state.mlp.predict(tweet_vector)[0]
            st.info("â¡ï¸ Ø§Ù„ØªØºØ±ÙŠØ¯Ø© Ø¥ÙŠØ¬Ø§Ø¨ÙŠØ©" if pred == 1 else "â¡ï¸ Ø§Ù„ØªØºØ±ÙŠØ¯Ø© Ø³Ù„Ø¨ÙŠØ©")
    else:
        st.warning("âš ï¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ØºÙŠØ± Ù…Ø¯Ø±Ø¨ Ø¨Ø¹Ø¯")

st.markdown("---")

# ---------------------------
# ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¯Ø®ÙˆÙ„ Ù„Ù„Ù…Ø¯ÙŠØ±
# ---------------------------
if not st.session_state.logged_in:
    st.subheader("ğŸ”’ ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¯Ø®ÙˆÙ„ Ù„Ù„Ù…Ø¯ÙŠØ±")
    username = st.text_input("Ø§Ø³Ù… Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…", key="admin_user")
    password = st.text_input("ÙƒÙ„Ù…Ø© Ø§Ù„Ù…Ø±ÙˆØ±", type="password", key="admin_pass")
    if st.button("Ø¯Ø®ÙˆÙ„", key="login_btn"):
        if username == "admin" and password == "1234":
            st.session_state.logged_in = True
            st.success("ØªÙ… ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¯Ø®ÙˆÙ„ Ø¨Ù†Ø¬Ø§Ø­!")
        else:
            st.error("Ø§Ø³Ù… Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø£Ùˆ ÙƒÙ„Ù…Ø© Ø§Ù„Ù…Ø±ÙˆØ± ØºÙŠØ± ØµØ­ÙŠØ­Ø©")

# ---------------------------
# ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù…Ø¯ÙŠØ±
# ---------------------------
if st.session_state.logged_in:
    st.title("ğŸ‘¨â€ğŸ’¼ ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù…Ø¯ÙŠØ±")
    st.subheader("Ø±ÙØ¹ Ù…Ù„ÙØ§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ CSV/TSV")
    file = st.file_uploader("Ø§Ø±ÙØ¹ Ù…Ù„Ù CSV Ø£Ùˆ TSV Ù„Ù„ØªØ¯Ø±ÙŠØ¨", type=["csv","tsv"], key="train_file")
    file_lang = st.radio("Ø§Ø®ØªØ± Ù„ØºØ© Ø§Ù„Ù…Ù„Ù", ["Arabic","English"], key="file_lang")

    if file:
        sep = "\t" if file.name.endswith(".tsv") else ","
        df = pd.read_csv(file, sep=sep, header=None, names=["label","text"])
        df = df.dropna(subset=["label","text"])
        st.success(f"âœ… ØªÙ… Ø±ÙØ¹ Ø§Ù„Ù…Ù„Ù Ø¨Ù†Ø¬Ø§Ø­: {file.name} ({len(df)} Ø³Ø·Ø±)")

        if file_lang == "Arabic":
            df = df[df['text'].str.contains(r'[\u0600-\u06FF]', na=False)]
        df["clean_text"] = df["text"].apply(lambda x: clean_text(x, lang="ar" if file_lang=="Arabic" else "en"))
        df = df[df["clean_text"].str.strip() != ""]

        df_majority = df[df.label=="neg"]
        df_minority = df[df.label=="pos"]
        if len(df_minority) > 0 and len(df_majority) > 0:
            df_minority_upsampled = resample(df_minority, replace=True, n_samples=len(df_majority), random_state=42)
            df_balanced = pd.concat([df_majority, df_minority_upsampled])
        else:
            df_balanced = df.copy()

        if st.button("ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ù„Ù"):
            if df_balanced.empty:
                st.error("âš ï¸ Ù„Ø§ ØªÙˆØ¬Ø¯ Ù†ØµÙˆØµ ØµØ§Ù„Ø­Ø© Ù„Ù„ØªØ¯Ø±ÙŠØ¨ Ø¨Ø¹Ø¯ Ø§Ù„ØªÙ†Ø¸ÙŠÙ!")
            else:
                with st.spinner("â³ Ø¬Ø§Ø±ÙŠ Ø§Ù„ØªØ¯Ø±ÙŠØ¨..."):
                    vectorizer = TfidfVectorizer(max_features=15000, ngram_range=(1,2))
                    X_train = vectorizer.fit_transform(df_balanced["clean_text"])
                    y_train = df_balanced["label"].map({"neg":0,"pos":1})
                    mlp = MLPClassifier(hidden_layer_sizes=(150,50), max_iter=50, random_state=42)
                    mlp.fit(X_train, y_train)

                    st.session_state.mlp = mlp
                    st.session_state.vectorizer = vectorizer
                    joblib.dump(mlp, "mlp_model.pkl")
                    joblib.dump(vectorizer, "tfidf_vectorizer.pkl")
                    
                    # Ø±ÙØ¹ Ø§Ù„Ù…Ù„ÙØ§Øª Ø¥Ù„Ù‰ Google Drive
                    upload_to_drive("mlp_model.pkl")
                    upload_to_drive("tfidf_vectorizer.pkl")

                    st.success("âœ… ØªÙ… Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ±ÙØ¹ Ø§Ù„Ù…Ù„ÙØ§Øª Ø¥Ù„Ù‰ Google Drive!")

    st.markdown("---")
    st.subheader("ØªØ¬Ø±Ø¨Ø© Ø§Ù„ØªØºØ±ÙŠØ¯Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©")
    new_tweet = st.text_input("Ø§Ø¯Ø®Ù„ ØªØºØ±ÙŠØ¯Ø© Ù„Ù„ØªØµÙ†ÙŠÙ", key="new_tweet_admin")
    new_lang = st.radio("Ø§Ø®ØªØ± Ù„ØºØ© Ø§Ù„ØªØºØ±ÙŠØ¯Ø©", ["Arabic","English"], key="new_lang_admin")
    if st.button("ØµÙ†Ù Ø§Ù„ØªØºØ±ÙŠØ¯Ø©", key="predict_new"):
        if st.session_state.mlp and st.session_state.vectorizer:
            tweet_clean = clean_text(new_tweet, lang="ar" if new_lang=="Arabic" else "en")
            if tweet_clean.strip() == "":
                st.warning("âš ï¸ Ø§Ù„Ù†Øµ Ù„Ø§ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ ÙƒÙ„Ù…Ø§Øª ØµØ§Ù„Ø­Ø© Ù„Ù„ØªØµÙ†ÙŠÙ.")
            else:
                tweet_vector = st.session_state.vectorizer.transform([tweet_clean])
                pred = st.session_state.mlp.predict(tweet_vector)[0]
                st.info("â¡ï¸ Ø§Ù„ØªØºØ±ÙŠØ¯Ø© Ø¥ÙŠØ¬Ø§Ø¨ÙŠØ©" if pred == 1 else "â¡ï¸ Ø§Ù„ØªØºØ±ÙŠØ¯Ø© Ø³Ù„Ø¨ÙŠØ©")
        else:
            st.warning("âš ï¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ØºÙŠØ± Ù…Ø¯Ø±Ø¨ Ø¨Ø¹Ø¯")

    st.markdown("---")
    st.subheader("ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ù† ØªØºØ±ÙŠØ¯Ø© ÙˆØ§Ø­Ø¯Ø©")
    tweet_to_train = st.text_input("Ø§Ø¯Ø®Ù„ ØªØºØ±ÙŠØ¯Ø© Ù„Ù„ØªØ¯Ø±ÙŠØ¨", key="train_one_admin")
    y_label = st.radio("Ø§Ø®ØªØ± Ø§Ù„ØªØµÙ†ÙŠÙ Ù„Ù„ØªØºØ±ÙŠØ¯Ø©", ["pos","neg"], key="label_one_admin")
    if st.button("ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù‡Ø°Ù‡ Ø§Ù„ØªØºØ±ÙŠØ¯Ø©", key="train_one_btn"):
        if st.session_state.mlp and st.session_state.vectorizer:
            tweet_clean = clean_text(tweet_to_train, lang="ar" if new_lang=="Arabic" else "en")
            if tweet_clean.strip() == "":
                st.warning("âš ï¸ Ø§Ù„Ù†Øµ Ù„Ø§ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ ÙƒÙ„Ù…Ø§Øª ØµØ§Ù„Ø­Ø© Ù„Ù„ØªØ¯Ø±ÙŠØ¨.")
            else:
                X_new = st.session_state.vectorizer.transform([tweet_clean])
                y_new = [1 if y_label=="pos" else 0]
                st.session_state.mlp.partial_fit(X_new, y_new)
                joblib.dump(st.session_state.mlp, "mlp_model.pkl")
                joblib.dump(st.session_state.vectorizer, "tfidf_vectorizer.pkl")
                
                # Ø±ÙØ¹ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© Ø¥Ù„Ù‰ Google Drive
                upload_to_drive("mlp_model.pkl")
                upload_to_drive("tfidf_vectorizer.pkl")

                st.success("âœ… ØªÙ… ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ù„ØªØºØ±ÙŠØ¯Ø© ÙˆØ±ÙØ¹ Ø§Ù„Ù…Ù„ÙØ§Øª Ø¥Ù„Ù‰ Google Drive")

# ---------------------------
# Footer
# ---------------------------
st.markdown(
    """
    <div style='text-align:center; opacity:0.4; margin-top:30px;'>
        Â© 2025 Amin Al Gbri
    </div>
    """, unsafe_allow_html=True
)
